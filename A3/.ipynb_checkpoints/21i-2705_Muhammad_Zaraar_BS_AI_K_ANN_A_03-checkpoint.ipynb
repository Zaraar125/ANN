{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc8b1e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ec2792",
   "metadata": {},
   "source": [
    "# Class of Neural Network with Affine Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "24a48175",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.temp=0\n",
    "        self.weights=0\n",
    "        self.bias=0\n",
    "        self.outputs=0\n",
    "        self.derivative_table=0\n",
    "        self.model_loss=100\n",
    "        \n",
    "    def initialize_weights_and_bais(self,X):\n",
    "        \n",
    "        self.weights=np.array([np.random.random(X.shape[1]),np.random.random(X.shape[1]),np.random.random(X.shape[1])])\n",
    "        self.bias=np.round(np.random.random(self.weights.shape[0]),3)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        \n",
    "        self.outputs=np.dot(X,self.weights.T)+self.bias\n",
    "    \n",
    "    def backward(self,X,alpha=0.0001):\n",
    "        \n",
    "        self.weights=self.weights-alpha*(np.dot(self.derivative_table.T,X))\n",
    "        self.bias=self.bias-alpha*(np.sum(self.derivative_table,axis=0))\n",
    "    \n",
    "    def loss(self,y):\n",
    "        \n",
    "        self.derivative_table=self.outputs-self.outputs[np.arange(y.shape[0]),y].reshape(-1,1)\n",
    "        self.derivative_table[self.derivative_table<=0]=0\n",
    "        \n",
    "        self.model_loss=np.sum(self.derivative_table)\n",
    "        \n",
    "        self.derivative_table[self.derivative_table>0]=1\n",
    "        self.derivative_table[np.arange(y.shape[0]),y]=np.sum(self.derivative_table,axis=1)*-1\n",
    "        \n",
    "    def train(self,X,y,LR=0.001,iterations=50000,loss_time=1000):\n",
    "\n",
    "        self.initialize_weights_and_bais(X)  \n",
    "        c=0\n",
    "        while(self.model_loss>=0.1):\n",
    "            self.forward(X)\n",
    "            self.loss(y)\n",
    "            self.backward(X,LR)\n",
    "            if c%loss_time==0:\n",
    "                print(\"ITERATION : \",c,\" \\n    MODEL LOSS : \",self.model_loss)\n",
    "            c=c+1\n",
    "            \n",
    "    def test(self,X,y):\n",
    "        \n",
    "        self.forward(X)\n",
    "        temp=self.outputs\n",
    "        holder=[]\n",
    "        for i in temp:\n",
    "            i=list(i)\n",
    "            holder.append(i.index(max(i)))\n",
    "        tt = sum(1 for x, z in zip(holder, y) if x == z)\n",
    "        print((tt/len(holder))*100)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38074c03",
   "metadata": {},
   "source": [
    "# Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bf34bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=datasets.load_iris()\n",
    "x,y,names=data['data'],data['target'],data[\"target_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2af90f6",
   "metadata": {},
   "source": [
    "# Splitting the Dataset into Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a1fab90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.1,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56007ef",
   "metadata": {},
   "source": [
    "# Training the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cc096c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION :  0  \n",
      "    MODEL LOSS :  269.53097412631774\n",
      "ITERATION :  10  \n",
      "    MODEL LOSS :  27.175544624615846\n",
      "ITERATION :  20  \n",
      "    MODEL LOSS :  27.30951996317742\n",
      "ITERATION :  30  \n",
      "    MODEL LOSS :  15.698434219005708\n",
      "ITERATION :  40  \n",
      "    MODEL LOSS :  14.655411832427445\n",
      "ITERATION :  50  \n",
      "    MODEL LOSS :  13.955856832427589\n",
      "ITERATION :  60  \n",
      "    MODEL LOSS :  12.66534583242778\n",
      "ITERATION :  70  \n",
      "    MODEL LOSS :  10.687830832427876\n",
      "ITERATION :  80  \n",
      "    MODEL LOSS :  10.558559634426542\n",
      "ITERATION :  90  \n",
      "    MODEL LOSS :  11.891986514549473\n",
      "ITERATION :  100  \n",
      "    MODEL LOSS :  7.03084732436994\n",
      "ITERATION :  110  \n",
      "    MODEL LOSS :  7.278177324370013\n",
      "ITERATION :  120  \n",
      "    MODEL LOSS :  6.728399588453177\n",
      "ITERATION :  130  \n",
      "    MODEL LOSS :  4.659477311884556\n",
      "ITERATION :  140  \n",
      "    MODEL LOSS :  4.115416285546116\n",
      "ITERATION :  150  \n",
      "    MODEL LOSS :  4.3514188989679745\n",
      "ITERATION :  160  \n",
      "    MODEL LOSS :  3.949776953500333\n",
      "ITERATION :  170  \n",
      "    MODEL LOSS :  2.6833686201671725\n",
      "ITERATION :  180  \n",
      "    MODEL LOSS :  0.5006588865022668\n",
      "ITERATION :  190  \n",
      "    MODEL LOSS :  0.49373888650227293\n",
      "ITERATION :  200  \n",
      "    MODEL LOSS :  0.4868188865022809\n",
      "ITERATION :  210  \n",
      "    MODEL LOSS :  0.4798988865022835\n",
      "ITERATION :  220  \n",
      "    MODEL LOSS :  0.4729788865022879\n",
      "ITERATION :  230  \n",
      "    MODEL LOSS :  0.4660588865022852\n",
      "ITERATION :  240  \n",
      "    MODEL LOSS :  0.4630988865022889\n",
      "ITERATION :  250  \n",
      "    MODEL LOSS :  0.4550488865022846\n",
      "ITERATION :  260  \n",
      "    MODEL LOSS :  0.4520888865022865\n",
      "ITERATION :  270  \n",
      "    MODEL LOSS :  0.4443948670886577\n",
      "ITERATION :  280  \n",
      "    MODEL LOSS :  0.43502636781940573\n",
      "ITERATION :  290  \n",
      "    MODEL LOSS :  0.42961836781940654\n",
      "ITERATION :  300  \n",
      "    MODEL LOSS :  0.424210367819402\n",
      "ITERATION :  310  \n",
      "    MODEL LOSS :  0.4188023678194064\n",
      "ITERATION :  320  \n",
      "    MODEL LOSS :  0.41339436781940364\n",
      "ITERATION :  330  \n",
      "    MODEL LOSS :  0.40798636781939734\n",
      "ITERATION :  340  \n",
      "    MODEL LOSS :  0.40257836781939815\n",
      "ITERATION :  350  \n",
      "    MODEL LOSS :  0.39718888650230255\n",
      "ITERATION :  360  \n",
      "    MODEL LOSS :  0.3919688865023083\n",
      "ITERATION :  370  \n",
      "    MODEL LOSS :  0.38674888650231054\n",
      "ITERATION :  380  \n",
      "    MODEL LOSS :  0.3815288865023092\n",
      "ITERATION :  390  \n",
      "    MODEL LOSS :  0.3763088865023132\n",
      "ITERATION :  400  \n",
      "    MODEL LOSS :  0.37108888650232075\n",
      "ITERATION :  410  \n",
      "    MODEL LOSS :  0.3658688865023141\n",
      "ITERATION :  420  \n",
      "    MODEL LOSS :  0.3606488865023181\n",
      "ITERATION :  430  \n",
      "    MODEL LOSS :  0.3554288865023185\n",
      "ITERATION :  440  \n",
      "    MODEL LOSS :  0.35020888650232607\n",
      "ITERATION :  450  \n",
      "    MODEL LOSS :  0.3449888865023194\n",
      "ITERATION :  460  \n",
      "    MODEL LOSS :  0.3397688865023234\n",
      "ITERATION :  470  \n",
      "    MODEL LOSS :  0.33454888650232206\n",
      "ITERATION :  480  \n",
      "    MODEL LOSS :  0.33766836781939347\n",
      "ITERATION :  490  \n",
      "    MODEL LOSS :  0.3245783678193934\n",
      "ITERATION :  500  \n",
      "    MODEL LOSS :  0.3215445393494871\n",
      "ITERATION :  510  \n",
      "    MODEL LOSS :  0.32697836781939316\n",
      "ITERATION :  520  \n",
      "    MODEL LOSS :  0.3177443678193903\n",
      "ITERATION :  530  \n",
      "    MODEL LOSS :  0.31931236781938566\n",
      "ITERATION :  540  \n",
      "    MODEL LOSS :  0.3197583678193894\n",
      "ITERATION :  550  \n",
      "    MODEL LOSS :  0.2962183678193835\n",
      "ITERATION :  560  \n",
      "    MODEL LOSS :  0.3079983678193834\n",
      "ITERATION :  570  \n",
      "    MODEL LOSS :  0.3269238789319182\n",
      "ITERATION :  580  \n",
      "    MODEL LOSS :  0.28711503386871584\n",
      "ITERATION :  590  \n",
      "    MODEL LOSS :  0.2856326867159016\n",
      "ITERATION :  600  \n",
      "    MODEL LOSS :  0.2859943678193737\n",
      "ITERATION :  610  \n",
      "    MODEL LOSS :  0.2824103678193719\n",
      "ITERATION :  620  \n",
      "    MODEL LOSS :  0.2788263678193683\n",
      "ITERATION :  630  \n",
      "    MODEL LOSS :  0.2718003678193668\n",
      "ITERATION :  640  \n",
      "    MODEL LOSS :  0.2701650338687198\n",
      "ITERATION :  650  \n",
      "    MODEL LOSS :  0.26551303386872505\n",
      "ITERATION :  660  \n",
      "    MODEL LOSS :  0.26086103386872317\n",
      "ITERATION :  670  \n",
      "    MODEL LOSS :  0.259414367819355\n",
      "ITERATION :  680  \n",
      "    MODEL LOSS :  0.2577790338687347\n",
      "ITERATION :  690  \n",
      "    MODEL LOSS :  0.25312703386873814\n",
      "ITERATION :  700  \n",
      "    MODEL LOSS :  0.24847503386873093\n",
      "ITERATION :  710  \n",
      "    MODEL LOSS :  0.24702836781934856\n",
      "ITERATION :  720  \n",
      "    MODEL LOSS :  0.24551364296105227\n",
      "ITERATION :  730  \n",
      "    MODEL LOSS :  0.24802537966264637\n",
      "ITERATION :  740  \n",
      "    MODEL LOSS :  0.23816703386874316\n",
      "ITERATION :  750  \n",
      "    MODEL LOSS :  0.23479236781934\n",
      "ITERATION :  760  \n",
      "    MODEL LOSS :  0.259768379662642\n",
      "ITERATION :  770  \n",
      "    MODEL LOSS :  0.24277604882674098\n",
      "ITERATION :  780  \n",
      "    MODEL LOSS :  0.24403737966263783\n",
      "ITERATION :  790  \n",
      "    MODEL LOSS :  0.23242937966264066\n",
      "ITERATION :  800  \n",
      "    MODEL LOSS :  0.2309456429610428\n",
      "ITERATION :  810  \n",
      "    MODEL LOSS :  0.24530137966264753\n",
      "ITERATION :  820  \n",
      "    MODEL LOSS :  0.23112404882672166\n",
      "ITERATION :  830  \n",
      "    MODEL LOSS :  0.23040937966265673\n",
      "ITERATION :  840  \n",
      "    MODEL LOSS :  0.22459964296104928\n",
      "ITERATION :  850  \n",
      "    MODEL LOSS :  0.2299033796626624\n",
      "ITERATION :  860  \n",
      "    MODEL LOSS :  0.21846564296104276\n",
      "ITERATION :  870  \n",
      "    MODEL LOSS :  0.22939737966266271\n",
      "ITERATION :  880  \n",
      "    MODEL LOSS :  0.21233164296103801\n",
      "ITERATION :  890  \n",
      "    MODEL LOSS :  0.22889137966267192\n",
      "ITERATION :  900  \n",
      "    MODEL LOSS :  0.20682465480434686\n",
      "ITERATION :  910  \n",
      "    MODEL LOSS :  0.21011164296102613\n",
      "ITERATION :  920  \n",
      "    MODEL LOSS :  0.20575604571206085\n",
      "ITERATION :  930  \n",
      "    MODEL LOSS :  0.23601737966268388\n",
      "ITERATION :  940  \n",
      "    MODEL LOSS :  0.2023146548043453\n",
      "ITERATION :  950  \n",
      "    MODEL LOSS :  0.20364564296103183\n",
      "ITERATION :  960  \n",
      "    MODEL LOSS :  0.2013460457120555\n",
      "ITERATION :  970  \n",
      "    MODEL LOSS :  0.1994360457120532\n",
      "ITERATION :  980  \n",
      "    MODEL LOSS :  0.19752604571205623\n",
      "ITERATION :  990  \n",
      "    MODEL LOSS :  0.1953746548043469\n",
      "ITERATION :  1000  \n",
      "    MODEL LOSS :  0.19474465480434766\n",
      "ITERATION :  1010  \n",
      "    MODEL LOSS :  0.19411465480434842\n",
      "ITERATION :  1020  \n",
      "    MODEL LOSS :  0.19442804571206196\n",
      "ITERATION :  1030  \n",
      "    MODEL LOSS :  0.192518045712065\n",
      "ITERATION :  1040  \n",
      "    MODEL LOSS :  0.19060804571206091\n",
      "ITERATION :  1050  \n",
      "    MODEL LOSS :  0.18869804571205862\n",
      "ITERATION :  1060  \n",
      "    MODEL LOSS :  0.20338537966269854\n",
      "ITERATION :  1070  \n",
      "    MODEL LOSS :  0.20198737966269853\n",
      "ITERATION :  1080  \n",
      "    MODEL LOSS :  0.2011073796626981\n",
      "ITERATION :  1090  \n",
      "    MODEL LOSS :  0.20022737966270476\n",
      "ITERATION :  1100  \n",
      "    MODEL LOSS :  0.19882937966270653\n",
      "ITERATION :  1110  \n",
      "    MODEL LOSS :  0.19794937966270432\n",
      "ITERATION :  1120  \n",
      "    MODEL LOSS :  0.1808320457120658\n",
      "ITERATION :  1130  \n",
      "    MODEL LOSS :  0.19567137966271098\n",
      "ITERATION :  1140  \n",
      "    MODEL LOSS :  0.19479137966271054\n",
      "ITERATION :  1150  \n",
      "    MODEL LOSS :  0.19339337966271586\n",
      "ITERATION :  1160  \n",
      "    MODEL LOSS :  0.19251337966272075\n",
      "ITERATION :  1170  \n",
      "    MODEL LOSS :  0.1916333796627221\n",
      "ITERATION :  1180  \n",
      "    MODEL LOSS :  0.19023537966271675\n",
      "ITERATION :  1190  \n",
      "    MODEL LOSS :  0.18935537966272342\n",
      "ITERATION :  1200  \n",
      "    MODEL LOSS :  0.17191604571207364\n",
      "ITERATION :  1210  \n",
      "    MODEL LOSS :  0.215197379662726\n",
      "ITERATION :  1220  \n",
      "    MODEL LOSS :  0.1836307135171431\n",
      "ITERATION :  1230  \n",
      "    MODEL LOSS :  0.17242537966273552\n",
      "ITERATION :  1240  \n",
      "    MODEL LOSS :  0.26943075687346507\n",
      "ITERATION :  1250  \n",
      "    MODEL LOSS :  0.1664080457120818\n",
      "ITERATION :  1260  \n",
      "    MODEL LOSS :  0.18882522143619518\n",
      "ITERATION :  1270  \n",
      "    MODEL LOSS :  0.229779138618893\n",
      "ITERATION :  1280  \n",
      "    MODEL LOSS :  0.233135138618902\n",
      "ITERATION :  1290  \n",
      "    MODEL LOSS :  0.16657737966274766\n",
      "ITERATION :  1300  \n",
      "    MODEL LOSS :  0.167797379662753\n",
      "ITERATION :  1310  \n",
      "    MODEL LOSS :  0.18788922143617093\n",
      "ITERATION :  1320  \n",
      "    MODEL LOSS :  0.1711172214361607\n",
      "ITERATION :  1330  \n",
      "    MODEL LOSS :  0.23585513861891627\n",
      "ITERATION :  1340  \n",
      "    MODEL LOSS :  0.16517937966276186\n",
      "ITERATION :  1350  \n",
      "    MODEL LOSS :  0.183737221436159\n",
      "ITERATION :  1360  \n",
      "    MODEL LOSS :  0.1601387135170924\n",
      "ITERATION :  1370  \n",
      "    MODEL LOSS :  0.2352191386189233\n",
      "ITERATION :  1380  \n",
      "    MODEL LOSS :  0.16256137966276718\n",
      "ITERATION :  1390  \n",
      "    MODEL LOSS :  0.17958522143613997\n",
      "ITERATION :  1400  \n",
      "    MODEL LOSS :  0.2248991386189303\n",
      "ITERATION :  1410  \n",
      "    MODEL LOSS :  0.2345831386189321\n",
      "ITERATION :  1420  \n",
      "    MODEL LOSS :  0.15994337966278138\n",
      "ITERATION :  1430  \n",
      "    MODEL LOSS :  0.17543322143612805\n",
      "ITERATION :  1440  \n",
      "    MODEL LOSS :  0.22426313861893377\n",
      "ITERATION :  1450  \n",
      "    MODEL LOSS :  0.19768713861894227\n",
      "ITERATION :  1460  \n",
      "    MODEL LOSS :  0.15781737966276843\n",
      "ITERATION :  1470  \n",
      "    MODEL LOSS :  0.1704692214361252\n",
      "ITERATION :  1480  \n",
      "    MODEL LOSS :  0.2241631386189411\n",
      "ITERATION :  1490  \n",
      "    MODEL LOSS :  0.14995737966277645\n",
      "ITERATION :  1500  \n",
      "    MODEL LOSS :  0.14450604571208991\n",
      "ITERATION :  1510  \n",
      "    MODEL LOSS :  0.1611292214361164\n",
      "ITERATION :  1520  \n",
      "    MODEL LOSS :  0.22886625760436452\n",
      "ITERATION :  1530  \n",
      "    MODEL LOSS :  0.21786913861893709\n",
      "ITERATION :  1540  \n",
      "    MODEL LOSS :  0.2332002576043699\n",
      "ITERATION :  1550  \n",
      "    MODEL LOSS :  0.14050404571209718\n",
      "ITERATION :  1560  \n",
      "    MODEL LOSS :  0.14549337966277953\n",
      "ITERATION :  1570  \n",
      "    MODEL LOSS :  0.24290420959273717\n",
      "ITERATION :  1580  \n",
      "    MODEL LOSS :  0.15806722143611296\n",
      "ITERATION :  1590  \n",
      "    MODEL LOSS :  0.20190513861895099\n",
      "ITERATION :  1600  \n",
      "    MODEL LOSS :  0.139548045712095\n",
      "ITERATION :  1610  \n",
      "    MODEL LOSS :  0.20914913861896878\n",
      "ITERATION :  1620  \n",
      "    MODEL LOSS :  0.17283122143606455\n",
      "ITERATION :  1630  \n",
      "    MODEL LOSS :  0.24851020959267522\n",
      "ITERATION :  1640  \n",
      "    MODEL LOSS :  0.20824513861898453\n",
      "ITERATION :  1650  \n",
      "    MODEL LOSS :  0.17098722143604483\n",
      "ITERATION :  1660  \n",
      "    MODEL LOSS :  0.2461182095926464\n",
      "ITERATION :  1670  \n",
      "    MODEL LOSS :  0.13677661234381588\n",
      "ITERATION :  1680  \n",
      "    MODEL LOSS :  0.19215522143601937\n",
      "ITERATION :  1690  \n",
      "    MODEL LOSS :  0.20019313861901367\n",
      "ITERATION :  1700  \n",
      "    MODEL LOSS :  0.19799913861902319\n",
      "ITERATION :  1710  \n",
      "    MODEL LOSS :  0.160729221435977\n",
      "ITERATION :  1720  \n",
      "    MODEL LOSS :  0.13047204571211424\n",
      "ITERATION :  1730  \n",
      "    MODEL LOSS :  0.13497261234375912\n",
      "ITERATION :  1740  \n",
      "    MODEL LOSS :  0.16196113861905914\n",
      "ITERATION :  1750  \n",
      "    MODEL LOSS :  0.19358713861907084\n",
      "ITERATION :  1760  \n",
      "    MODEL LOSS :  0.12795004571211877\n",
      "ITERATION :  1770  \n",
      "    MODEL LOSS :  0.22380354913962996\n",
      "ITERATION :  1780  \n",
      "    MODEL LOSS :  0.13540461234370937\n",
      "ITERATION :  1790  \n",
      "    MODEL LOSS :  0.1947111386191036\n",
      "ITERATION :  1800  \n",
      "    MODEL LOSS :  0.19034513861910796\n",
      "ITERATION :  1810  \n",
      "    MODEL LOSS :  0.12404804571212935\n",
      "ITERATION :  1820  \n",
      "    MODEL LOSS :  0.30334220959239566\n",
      "ITERATION :  1830  \n",
      "    MODEL LOSS :  0.220969549139717\n",
      "ITERATION :  1840  \n",
      "    MODEL LOSS :  0.24361420959238167\n",
      "ITERATION :  1850  \n",
      "    MODEL LOSS :  0.13793722143581277\n",
      "ITERATION :  1860  \n",
      "    MODEL LOSS :  0.19061025760464645\n",
      "ITERATION :  1870  \n",
      "    MODEL LOSS :  0.15152513861914763\n",
      "ITERATION :  1880  \n",
      "    MODEL LOSS :  0.15468913861914935\n",
      "ITERATION :  1890  \n",
      "    MODEL LOSS :  0.15182513861915758\n",
      "ITERATION :  1900  \n",
      "    MODEL LOSS :  0.1549891386191664\n",
      "ITERATION :  1910  \n",
      "    MODEL LOSS :  0.15212513861917287\n",
      "ITERATION :  1920  \n",
      "    MODEL LOSS :  0.15528913861917104\n",
      "ITERATION :  1930  \n",
      "    MODEL LOSS :  0.2538042095922872\n",
      "ITERATION :  1940  \n",
      "    MODEL LOSS :  0.2466542095922719\n",
      "ITERATION :  1950  \n",
      "    MODEL LOSS :  0.2511522095922576\n",
      "ITERATION :  1960  \n",
      "    MODEL LOSS :  0.11920620647808988\n",
      "ITERATION :  1970  \n",
      "    MODEL LOSS :  0.13832922143568283\n",
      "ITERATION :  1980  \n",
      "    MODEL LOSS :  0.1419692214356818\n",
      "ITERATION :  1990  \n",
      "    MODEL LOSS :  0.13669061234354807\n",
      "ITERATION :  2000  \n",
      "    MODEL LOSS :  0.11784420647806648\n",
      "ITERATION :  2010  \n",
      "    MODEL LOSS :  0.13690061234351703\n",
      "ITERATION :  2020  \n",
      "    MODEL LOSS :  0.3021702095921359\n",
      "ITERATION :  2030  \n",
      "    MODEL LOSS :  0.1213102064780518\n",
      "ITERATION :  2040  \n",
      "    MODEL LOSS :  0.11838220647804043\n",
      "ITERATION :  2050  \n",
      "    MODEL LOSS :  0.16011122143560463\n",
      "ITERATION :  2060  \n",
      "    MODEL LOSS :  0.15422722143559398\n",
      "ITERATION :  2070  \n",
      "    MODEL LOSS :  0.13616661234346772\n",
      "ITERATION :  2080  \n",
      "    MODEL LOSS :  0.11678820647801302\n",
      "ITERATION :  2090  \n",
      "    MODEL LOSS :  0.136376612343442\n",
      "ITERATION :  2100  \n",
      "    MODEL LOSS :  0.11660820647799675\n",
      "ITERATION :  2110  \n",
      "    MODEL LOSS :  0.16264722143551502\n",
      "ITERATION :  2120  \n",
      "    MODEL LOSS :  0.1567632214355097\n",
      "ITERATION :  2130  \n",
      "    MODEL LOSS :  0.15087922143549548\n",
      "ITERATION :  2140  \n",
      "    MODEL LOSS :  0.14499522143548482\n",
      "ITERATION :  2150  \n",
      "    MODEL LOSS :  0.13911122143547416\n",
      "ITERATION :  2160  \n",
      "    MODEL LOSS :  0.2562642095919454\n",
      "ITERATION :  2170  \n",
      "    MODEL LOSS :  0.2491142095919301\n",
      "ITERATION :  2180  \n",
      "    MODEL LOSS :  0.11645020647795867\n",
      "ITERATION :  2190  \n",
      "    MODEL LOSS :  0.1370006123433587\n",
      "ITERATION :  2200  \n",
      "    MODEL LOSS :  0.1162702064779424\n",
      "ITERATION :  2210  \n",
      "    MODEL LOSS :  0.150253221435392\n",
      "ITERATION :  2220  \n",
      "    MODEL LOSS :  0.2668702095918345\n",
      "ITERATION :  2230  \n",
      "    MODEL LOSS :  0.25752220959183525\n",
      "ITERATION :  2240  \n",
      "    MODEL LOSS :  0.31590020959181686\n",
      "ITERATION :  2250  \n",
      "    MODEL LOSS :  0.12358261234330392\n",
      "ITERATION :  2260  \n",
      "    MODEL LOSS :  0.16078522143535068\n",
      "ITERATION :  2270  \n",
      "    MODEL LOSS :  0.20437122143532882\n",
      "ITERATION :  2280  \n",
      "    MODEL LOSS :  0.2562982095917725\n",
      "ITERATION :  2290  \n",
      "    MODEL LOSS :  0.12458861234327223\n",
      "ITERATION :  2300  \n",
      "    MODEL LOSS :  0.11886061234327272\n",
      "ITERATION :  2310  \n",
      "    MODEL LOSS :  0.14754922143531068\n",
      "ITERATION :  2320  \n",
      "    MODEL LOSS :  0.2644222095917268\n",
      "ITERATION :  2330  \n",
      "    MODEL LOSS :  0.3162502095917201\n",
      "ITERATION :  2340  \n",
      "    MODEL LOSS :  0.12389261234324067\n",
      "ITERATION :  2350  \n",
      "    MODEL LOSS :  0.1604352214352751\n",
      "ITERATION :  2360  \n",
      "    MODEL LOSS :  0.2040212214352568\n",
      "ITERATION :  2370  \n",
      "    MODEL LOSS :  0.2566482095916687\n",
      "ITERATION :  2380  \n",
      "    MODEL LOSS :  0.12489861234321786\n",
      "ITERATION :  2390  \n",
      "    MODEL LOSS :  0.11618420647785221\n",
      "ITERATION :  2400  \n",
      "    MODEL LOSS :  0.13843661234319526\n",
      "ITERATION :  2410  \n",
      "    MODEL LOSS :  0.12100661234318899\n",
      "ITERATION :  2420  \n",
      "    MODEL LOSS :  0.2024552214351889\n",
      "ITERATION :  2430  \n",
      "    MODEL LOSS :  0.1254166123431677\n",
      "ITERATION :  2440  \n",
      "    MODEL LOSS :  0.11644861234317005\n",
      "ITERATION :  2450  \n",
      "    MODEL LOSS :  0.2015332214351595\n",
      "ITERATION :  2460  \n",
      "    MODEL LOSS :  0.1248846123431484\n",
      "ITERATION :  2470  \n",
      "    MODEL LOSS :  0.15702522143516084\n",
      "ITERATION :  2480  \n",
      "    MODEL LOSS :  0.2006112214351372\n",
      "ITERATION :  2490  \n",
      "    MODEL LOSS :  0.12435261234313266\n",
      "ITERATION :  2500  \n",
      "    MODEL LOSS :  0.14849722143513056\n",
      "ITERATION :  2510  \n",
      "    MODEL LOSS :  0.26144020959150716\n",
      "ITERATION :  2520  \n",
      "    MODEL LOSS :  0.12382061234310981\n",
      "ITERATION :  2530  \n",
      "    MODEL LOSS :  0.14757522143510116\n",
      "ITERATION :  2540  \n",
      "    MODEL LOSS :  0.2605442095914796\n",
      "ITERATION :  2550  \n",
      "    MODEL LOSS :  0.1232886123430923\n",
      "ITERATION :  2560  \n",
      "    MODEL LOSS :  0.20447722143507185\n",
      "ITERATION :  2570  \n",
      "    MODEL LOSS :  0.2530982095914549\n",
      "ITERATION :  2580  \n",
      "    MODEL LOSS :  0.12275661234307123\n",
      "ITERATION :  2590  \n",
      "    MODEL LOSS :  0.20355522143504778\n",
      "ITERATION :  2600  \n",
      "    MODEL LOSS :  0.24276620959142114\n",
      "ITERATION :  2610  \n",
      "    MODEL LOSS :  0.1222246123430537\n",
      "ITERATION :  2620  \n",
      "    MODEL LOSS :  0.20263322143502016\n",
      "ITERATION :  2630  \n",
      "    MODEL LOSS :  0.1266346123430413\n",
      "ITERATION :  2640  \n",
      "    MODEL LOSS :  0.11766661234303655\n",
      "ITERATION :  2650  \n",
      "    MODEL LOSS :  0.20171122143499431\n",
      "ITERATION :  2660  \n",
      "    MODEL LOSS :  0.126102612343022\n",
      "ITERATION :  2670  \n",
      "    MODEL LOSS :  0.15720322143498144\n",
      "ITERATION :  2680  \n",
      "    MODEL LOSS :  0.20078922143496847\n",
      "ITERATION :  2690  \n",
      "    MODEL LOSS :  0.1255706123430027\n",
      "ITERATION :  2700  \n",
      "    MODEL LOSS :  0.14867522143495648\n",
      "ITERATION :  2710  \n",
      "    MODEL LOSS :  0.2632342095912925\n",
      "ITERATION :  2720  \n",
      "    MODEL LOSS :  0.1250386123429834\n",
      "ITERATION :  2730  \n",
      "    MODEL LOSS :  0.20557722143491475\n",
      "ITERATION :  2740  \n",
      "    MODEL LOSS :  0.25578820959127313\n",
      "ITERATION :  2750  \n",
      "    MODEL LOSS :  0.1245066123429659\n",
      "ITERATION :  2760  \n",
      "    MODEL LOSS :  0.20465522143490134\n",
      "ITERATION :  2770  \n",
      "    MODEL LOSS :  0.2581322095912153\n",
      "ITERATION :  2780  \n",
      "    MODEL LOSS :  0.12190861234293493\n",
      "ITERATION :  2790  \n",
      "    MODEL LOSS :  0.2011492214348607\n",
      "ITERATION :  2800  \n",
      "    MODEL LOSS :  0.12631861234292785\n",
      "ITERATION :  2810  \n",
      "    MODEL LOSS :  0.11528461234291498\n",
      "ITERATION :  2820  \n",
      "    MODEL LOSS :  0.19764322143481827\n",
      "ITERATION :  2830  \n",
      "    MODEL LOSS :  0.12372061234289689\n",
      "ITERATION :  2840  \n",
      "    MODEL LOSS :  0.20335322143480994\n",
      "ITERATION :  2850  \n",
      "    MODEL LOSS :  0.2568902095911092\n",
      "ITERATION :  2860  \n",
      "    MODEL LOSS :  0.12514861234286911\n",
      "ITERATION :  2870  \n",
      "    MODEL LOSS :  0.20547620959108137\n",
      "ITERATION :  2880  \n",
      "    MODEL LOSS :  0.19597522143474144\n",
      "ITERATION :  2890  \n",
      "    MODEL LOSS :  0.12255061234283637\n",
      "ITERATION :  2900  \n",
      "    MODEL LOSS :  0.201685221434726\n",
      "ITERATION :  2910  \n",
      "    MODEL LOSS :  0.2484782095910152\n",
      "ITERATION :  2920  \n",
      "    MODEL LOSS :  0.1239786123428086\n",
      "ITERATION :  2930  \n",
      "    MODEL LOSS :  0.2036142095909863\n",
      "ITERATION :  2940  \n",
      "    MODEL LOSS :  0.2639222095909517\n",
      "ITERATION :  2950  \n",
      "    MODEL LOSS :  0.12540661234277373\n",
      "ITERATION :  2960  \n",
      "    MODEL LOSS :  0.12019661234276136\n",
      "ITERATION :  2970  \n",
      "    MODEL LOSS :  0.19840722143462663\n",
      "ITERATION :  2980  \n",
      "    MODEL LOSS :  0.12460661234275427\n",
      "ITERATION :  2990  \n",
      "    MODEL LOSS :  0.1589212214346123\n",
      "ITERATION :  3000  \n",
      "    MODEL LOSS :  0.1971632214346002\n",
      "ITERATION :  3010  \n",
      "    MODEL LOSS :  0.1238066123427295\n",
      "ITERATION :  3020  \n",
      "    MODEL LOSS :  0.15767722143458585\n",
      "ITERATION :  3030  \n",
      "    MODEL LOSS :  0.1959192214345684\n",
      "ITERATION :  3040  \n",
      "    MODEL LOSS :  0.12300661234270471\n",
      "ITERATION :  3050  \n",
      "    MODEL LOSS :  0.15643322143455762\n",
      "ITERATION :  3060  \n",
      "    MODEL LOSS :  0.20005820959079657\n",
      "ITERATION :  3070  \n",
      "    MODEL LOSS :  0.2632522095907781\n",
      "ITERATION :  3080  \n",
      "    MODEL LOSS :  0.25476220959077445\n",
      "ITERATION :  3090  \n",
      "    MODEL LOSS :  0.12386461234265056\n",
      "ITERATION :  3100  \n",
      "    MODEL LOSS :  0.1186546123426453\n",
      "ITERATION :  3110  \n",
      "    MODEL LOSS :  0.15570322143447335\n",
      "ITERATION :  3120  \n",
      "    MODEL LOSS :  0.19946820959068923\n",
      "ITERATION :  3130  \n",
      "    MODEL LOSS :  0.2626622095906743\n",
      "ITERATION :  3140  \n",
      "    MODEL LOSS :  0.25417220959066\n",
      "ITERATION :  3150  \n",
      "    MODEL LOSS :  0.2427962095906384\n",
      "ITERATION :  3160  \n",
      "    MODEL LOSS :  0.1223546123425745\n",
      "ITERATION :  3170  \n",
      "    MODEL LOSS :  0.15258261234256665\n",
      "ITERATION :  3180  \n",
      "    MODEL LOSS :  0.2529862095905919\n",
      "ITERATION :  3190  \n",
      "    MODEL LOSS :  0.12276861234254888\n",
      "ITERATION :  3200  \n",
      "    MODEL LOSS :  0.11755861234253473\n",
      "ITERATION :  3210  \n",
      "    MODEL LOSS :  0.15404322143433546\n",
      "ITERATION :  3220  \n",
      "    MODEL LOSS :  0.19769220959053158\n",
      "ITERATION :  3230  \n",
      "    MODEL LOSS :  0.1300126123425045\n",
      "ITERATION :  3240  \n",
      "    MODEL LOSS :  0.11678861234250171\n",
      "ITERATION :  3250  \n",
      "    MODEL LOSS :  0.19844620959048775\n",
      "ITERATION :  3260  \n",
      "    MODEL LOSS :  0.26164020959046397\n",
      "ITERATION :  3270  \n",
      "    MODEL LOSS :  0.13143261234247205\n",
      "ITERATION :  3280  \n",
      "    MODEL LOSS :  0.20863620959044837\n",
      "ITERATION :  3290  \n",
      "    MODEL LOSS :  0.1973792214342236\n",
      "ITERATION :  3300  \n",
      "    MODEL LOSS :  0.2539042095904076\n",
      "ITERATION :  3310  \n",
      "    MODEL LOSS :  0.24252820959038957\n",
      "ITERATION :  3320  \n",
      "    MODEL LOSS :  0.12208661234241269\n",
      "ITERATION :  3330  \n",
      "    MODEL LOSS :  0.1523146123424155\n",
      "ITERATION :  3340  \n",
      "    MODEL LOSS :  0.25271820959035196\n",
      "ITERATION :  3350  \n",
      "    MODEL LOSS :  0.12250061234239062\n",
      "ITERATION :  3360  \n",
      "    MODEL LOSS :  0.11729061234237825\n",
      "ITERATION :  3370  \n",
      "    MODEL LOSS :  0.15374322143412655\n",
      "ITERATION :  3380  \n",
      "    MODEL LOSS :  0.1974242095902703\n"
     ]
    }
   ],
   "source": [
    "z=NeuralNetwork()\n",
    "z.train(x_train,y_train,0.0001,100,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2d7557",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fe79c6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "z.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15ca93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd554798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
